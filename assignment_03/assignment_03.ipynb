{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 5560\n"
     ]
    }
   ],
   "source": [
    "# Read in the csv file of known malware containing SHA256 hashes and corresponding malware family name\n",
    "# Save the hashes and family names in a dictionary\n",
    "\n",
    "# Define a function to return the csv containing the hashes and family for each malware sample in the dataset\n",
    "def read_malware_csv(file_path):\n",
    "    # Read each line of the csv file\n",
    "    with open(file_path, mode='r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # Skip the first row containing field names\n",
    "        next(reader)\n",
    "        # Split the line into a pair-value and add into a dictionary\n",
    "        malware_dict = {rows[0]: rows[1] for rows in reader}\n",
    "    return malware_dict\n",
    "\n",
    "# Read the csv\n",
    "malware_dict = read_malware_csv('sha256_family.csv')\n",
    "# Print the number of unique hashes\n",
    "print(\"Num samples: \" + str(len(malware_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature pre-processing\n",
    "# Create 2 new dictionaries: one to store feature strings found in malware files, and one to store feature strings found in all files\n",
    "\n",
    "# For each file in the dataset directory:\n",
    "# - For each line starting with 'feature' or 'permission' after splitting the line by '::':\n",
    "#   - Add the feature string to the dictionary of features found in all files\n",
    "#   - If the filename hash is contained in the malware dictionary, add the feature string to the dictionary of features found in malware files\n",
    "\n",
    "def process_features(directory_path, malware_dict):\n",
    "    all_features = {}\n",
    "    malware_features = {}\n",
    "    \n",
    "    # For each file in the dataset directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Join the directory path and filename to get the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Extract features and permissions from lines starting with 'feature' or 'permission'\n",
    "                if line.startswith('feature') or line.startswith('permission'):\n",
    "                    feature = line.split('::')[1].strip()\n",
    "                    # Count occurrences of each feature in all files\n",
    "                    all_features[feature] = all_features.get(feature, 0) + 1\n",
    "                    # Increment to malware features if the filename is in the malware dictionary\n",
    "                    if filename in malware_dict:\n",
    "                        malware_features[feature] = malware_features.get(feature, 0) + 1\n",
    "\n",
    "    return all_features, malware_features\n",
    "\n",
    "# Specify directory path containing the dataset (substitute with your own path)\n",
    "# directory_path = '<path_to_dataset_directory>'\n",
    "directory_path = 'C:\\\\Users\\\\Kevin\\\\Code\\\\GitHub\\\\CSEC620-ML\\\\assignment_03\\\\feature_vectors\\\\feature_vectors'\n",
    "all_features, malware_features = process_features(directory_path, malware_dict)\n",
    "\n",
    "# Generate the feature vectors with a specified size\n",
    "feature_vector_size = 25\n",
    "\n",
    "# Create a list of the most common malware features\n",
    "sorted_malware_features = sorted(malware_features.items(), key=lambda x: x[1], reverse=True)[:feature_vector_size]\n",
    "\n",
    "# Create a list of the most common malware features\n",
    "common_malware_features = [feature for feature, count in sorted_malware_features]\n",
    "\n",
    "# Create a dictionary of the most common unique features that are not common malware features \n",
    "unique_features = {feature: count for feature, count in all_features.items() if feature not in common_malware_features}\n",
    "sorted_unique_features = sorted(unique_features.items(), key=lambda x: x[1], reverse=True)[:feature_vector_size]\n",
    "\n",
    "# Generate a list containing the names of features from the sorted_malware_features dict and sorted_unique_features dict\n",
    "reference_vector = [feature for feature, count in sorted_malware_features] + [feature for feature, count in sorted_unique_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 129013\n"
     ]
    }
   ],
   "source": [
    "# Generate the vectorized dataset\n",
    "# For each file in the dataset directory:\n",
    "# - Create a vector of zeros with the same length as the reference vector\n",
    "# - For each line starting with 'feature' or 'permission' after splitting the line by '::':\n",
    "#   - If the feature is in the reference vector, set the corresponding index in the vector to 1\n",
    "# - Append the vector to the dataset list\n",
    "# - Save the dataset to a csv file\n",
    "\n",
    "def generate_vectorized_dataset(directory_path, reference_vector_dict, malware_dict):\n",
    "    dataset = []\n",
    "    dataset_hashes = []\n",
    "    dataset_labels = []\n",
    "    dataset_families = []\n",
    "\n",
    "    # For each file in the dataset directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Join the directory path and filename to get the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Create a vector of zeros with the same length as the reference vector\n",
    "        vector = np.zeros(len(reference_vector_dict), dtype=int)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Extract features and permissions from lines starting with 'feature' or 'permission'\n",
    "                if line.startswith('feature') or line.startswith('permission'):\n",
    "                    feature = line.split('::')[1].strip()\n",
    "                    # If the feature is in the reference vector, set the corresponding index in the vector to 1\n",
    "                    if feature in reference_vector_dict:\n",
    "                        vector[reference_vector_dict[feature]] = 1\n",
    "\n",
    "        # Append the filename hash to the dataset hashes list\n",
    "        dataset_hashes.append(filename)\n",
    "\n",
    "        # Append the label to the dataset labels list\n",
    "        if filename in malware_dict:\n",
    "            dataset_labels.append(1)\n",
    "            dataset_families.append(malware_dict[filename])\n",
    "        else:\n",
    "            dataset_labels.append(0)\n",
    "            dataset_families.append('benign')\n",
    "\n",
    "        # Append the vector to the dataset list\n",
    "        dataset.append(vector)\n",
    "\n",
    "    return dataset, dataset_hashes, dataset_labels, dataset_families\n",
    "\n",
    "# Convert the list into a dict with the order of the features in the list as the key\n",
    "reference_vector_dict = {feature: i for i, feature in enumerate(reference_vector)}\n",
    "\n",
    "# Generate the vectorized dataset\n",
    "dataset, dataset_hashes, dataset_labels, dataset_families = generate_vectorized_dataset(directory_path, reference_vector_dict, malware_dict)\n",
    "\n",
    "# Print the number of samples in the dataset\n",
    "print(\"Num samples: \" + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a csv file\n",
    "# Each row of the csv file will contain the hash, label, family, and feature vector for each sample\n",
    "def save_dataset_to_csv(dataset_hashes, dataset_labels, dataset_families, dataset, output_file_path):\n",
    "    df = pd.DataFrame(dataset_hashes, columns=['hash'])\n",
    "    # Add the hashes, labels, and families as new columns\n",
    "    df.insert(1, 'label', dataset_labels)\n",
    "    df.insert(2, 'family', dataset_families)\n",
    "    df.insert(3, 'vector', dataset)\n",
    "    # Save the DataFrame to a csv file\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Save the dataset to a csv file\n",
    "output_file_path = 'vectorized_dataset.csv'\n",
    "save_dataset_to_csv(dataset_hashes, dataset_labels, dataset_families, dataset, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded dataset from csv file:\n",
      "                                                hash  label     family  \\\n",
      "0  00002d74a9faa53f5199c910b652ef09d3a7f6bd42b693...      1  GinMaster   \n",
      "1  000068216bdb459df847bfdd67dd11069c3c50166db1ea...      0     benign   \n",
      "2  0000764713b286cfe7e8e76c7038c92312977712d9c5a8...      1     Opfake   \n",
      "3  0000962c2c34de1ca0c329b18be7847459da2d9d14b6b2...      0     benign   \n",
      "4  000167f1ff061ea91440c40659c11c2af160342fd2e493...      0     benign   \n",
      "\n",
      "                                              vector  \n",
      "0  [1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0...  \n",
      "1  [1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...  \n",
      "2  [1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0...  \n",
      "3  [1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0...  \n",
      "4  [1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0...  \n",
      "\n",
      "Keys of the loaded dataset:\n",
      "Index(['hash', 'label', 'family', 'vector'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the csv file\n",
    "def load_dataset_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Load the dataset from the csv file\n",
    "loaded_dataset = load_dataset_from_csv('vectorized_dataset.csv')\n",
    "print(\"\\nLoaded dataset from csv file:\")\n",
    "print(loaded_dataset.head())\n",
    "\n",
    "# Print keys of the loaded dataset\n",
    "print(\"\\nKeys of the loaded dataset:\")\n",
    "print(loaded_dataset.keys())\n",
    "\n",
    "# Convert all columns to numpy arrays\n",
    "dataset_hashes = loaded_dataset['hash'].tolist()\n",
    "dataset_labels = loaded_dataset['label'].tolist()\n",
    "dataset_families = loaded_dataset['family'].tolist()\n",
    "\n",
    "# Process each vector in the dataset by removing the brackets, splitting into a list via spaces, and typecasting to int\n",
    "dataset_vectors = []\n",
    "for vector in loaded_dataset['vector']:\n",
    "    vector = vector.strip('[]').split(' ')\n",
    "    # Typecast the vector of strings to int\n",
    "    vector = [int(i) for i in vector]\n",
    "    dataset_vectors.append(vector)\n",
    "dataset_vectors = dataset_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training sets created: 55\n"
     ]
    }
   ],
   "source": [
    "# Generate a one-vs-all training set for each family in the dataset with more than n occurrences\n",
    "\n",
    "# Set to 0 to create a dataset for all available families\n",
    "min_occurrences = 10\n",
    "\n",
    "# Create a dictionary to store the training sets for each family\n",
    "training_sets_samples = {}\n",
    "training_sets_labels = {}\n",
    "\n",
    "# Iterate through each family and create a one-vs-all training set for each\n",
    "family_counts = pd.Series(dataset_families).value_counts()\n",
    "for family in family_counts.index:\n",
    "    if family_counts[family] >= min_occurrences:\n",
    "        training_set_samples = []\n",
    "        training_set_labels = []\n",
    "        # Append malware samples for the current family\n",
    "        malware_indexes = [i for i in range(len(dataset_families)) if dataset_families[i] == family]\n",
    "        index = 0\n",
    "        # Create a balanced dataset with 50,000 malware samples and 50,000 benign samples\n",
    "        while len(training_set_samples) < 50000:\n",
    "            training_set_samples.append(dataset_vectors[malware_indexes[index]])\n",
    "            training_set_labels.append(1)\n",
    "            index += 1\n",
    "            if index >= len(malware_indexes):\n",
    "                index = 0\n",
    "        # Append benign samples for the current family\n",
    "        benign_indexes = [i for i in range(len(dataset_families)) if dataset_families[i] != family]\n",
    "        index = 0\n",
    "        while len(training_set_samples) < 100000:\n",
    "            training_set_samples.append(dataset_vectors[benign_indexes[index]])\n",
    "            training_set_labels.append(0)\n",
    "            index += 1\n",
    "            if index >= len(benign_indexes):\n",
    "                index = 0\n",
    "        training_sets_samples[family] = training_set_samples\n",
    "        training_sets_labels[family] = training_set_labels\n",
    "\n",
    "# Print the number of training sets created\n",
    "print(\"\\nNumber of training sets created:\", len(training_sets_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single class (one vs all) SVM implementation\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, epochs=2000, learning_rate=0.005):\n",
    "        # Set learning rate and number of epochs/iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Initiate weights and biases to None, assigned based on the size of the first training point\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, samples, raw_labels, regularization_term=0):\n",
    "        # Convert samples and labels to PyTorch tensors\n",
    "        samples = torch.tensor(samples, dtype=torch.float32)\n",
    "        raw_labels = torch.tensor(raw_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Update labels from (0 or 1) to (-1 or 1) for hinge loss calculation\n",
    "        labels = torch.where(raw_labels == 1, torch.tensor(1.0, dtype=torch.float32), torch.tensor(-1.0, dtype=torch.float32))\n",
    "\n",
    "        # Get the number of samples and number of features per sample\n",
    "        num_samples, num_features = samples.shape\n",
    "\n",
    "        # Initialize n weights to 0, where n is the number of features\n",
    "        # requires_grad set to True to allow automatic tuning by PyTorch optimizer\n",
    "        self.weights = torch.zeros((num_features, 1), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Set the initial bias to 0 (tensor of size 1)\n",
    "        # requires_grad set to True to allow automatic tuning by PyTorch optimizer\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Specify Stocastic Gradient Descent as the optimizer\n",
    "        optimizer = torch.optim.SGD([self.weights, self.bias], lr=self.learning_rate)\n",
    "\n",
    "        # Training function\n",
    "        for epoch_index in range(self.epochs):\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predict the score of the sample (same as predict function w/o function call overhead)\n",
    "            prediction = torch.matmul(samples, self.weights) - self.bias\n",
    "\n",
    "            # Compute the margin by applying the labels to the corresponding predictions\n",
    "            margin = prediction * labels\n",
    "\n",
    "            # Compute the average hinge loss\n",
    "            hinge_loss = torch.mean(torch.clamp(1 - margin, min=0))\n",
    "\n",
    "            # Add regularization to the cost function\n",
    "            reg_cost = regularization_term * torch.norm(self.weights, p=2) / 2\n",
    "\n",
    "            # Calculate total loss\n",
    "            loss = hinge_loss + reg_cost\n",
    "\n",
    "            # Use PyTorch to automatically compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "    def predict(self, samples):\n",
    "        # Compute the score/prediction by finding the dot product between the data points and the weights, including the bias\n",
    "        # Apply a sigmoid to the prediction to convert it to a probability for the multi-class ensemble classification\n",
    "        return torch.sigmoid(torch.matmul(torch.tensor(samples, dtype=torch.float32), self.weights) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble classifier which initializes multiple one vs all SVM classifiers, including one for benign samples\n",
    "# Each SVM classifier makes a prediction on the sample data and outputs a score\n",
    "# The classifier with the highest score classifies the sample\n",
    "\n",
    "class Ensemble_SVM:\n",
    "    def __init__(self, epochs=2000, learning_rate=0.005):\n",
    "        # Store the number of epochs and learning rate\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        # Set the number of models in the ensemble\n",
    "        self.num_models = None\n",
    "        # Initialize a list to store the models and a list for the corresponding family names\n",
    "        self.models = []\n",
    "        self.model_families = []\n",
    "    \n",
    "    def fit(self, families, sample_sets, label_sets, regularization_term=0):\n",
    "        # Get the number of models\n",
    "        self.num_models = len(families)\n",
    "        for index in range(len(families)):\n",
    "            self.model_families.append(families[index])\n",
    "            # Create an SVM model for each family and train it\n",
    "            model = SVM(epochs=self.epochs, learning_rate=self.learning_rate)\n",
    "            model.fit(sample_sets[index], label_sets[index], regularization_term)\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, samples):\n",
    "        # Predict the class of the sample using voting (highest score)\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            predictions.append(model.predict(samples))\n",
    "        # Get the index of the model with the highest prediction\n",
    "        max_index = predictions.index(max(predictions))\n",
    "        return self.model_families[max_index]\n",
    "    \n",
    "    # Predict the class of the sample with weighted predictions\n",
    "    def weighted_predict(self, samples, bias):\n",
    "        # Predict the class of the sample using voting (highest score)\n",
    "        predictions = []\n",
    "        for model_index in range(len(self.models)):\n",
    "            # Apply the accuracy of the model as a bias to the prediction\n",
    "            prediction = self.models[model_index].predict(samples)\n",
    "            # predictions.append(self.models[model_index].predict(samples) * bias[model_index])\n",
    "            predictions.append(prediction + (prediction * bias[model_index]))\n",
    "        # Get the index of the model with the highest prediction\n",
    "        max_index = predictions.index(max(predictions))\n",
    "        return self.model_families[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 families used in the ensemble model:\n",
      "['benign', 'FakeInstaller', 'DroidKungFu', 'Plankton', 'Opfake', 'GinMaster', 'BaseBridge', 'Iconosys', 'Kmin', 'FakeDoc', 'Geinimi', 'Adrd', 'DroidDream', 'ExploitLinuxLotoor', 'Glodream', 'MobileTx', 'FakeRun', 'SendPay', 'Gappusin', 'Imlog']\n"
     ]
    }
   ],
   "source": [
    "# Train an ensemble SVM model using the top 20 families with the most occurrences\n",
    "\n",
    "# Get the top 20 families with the most occurrences and convert the names to a list\n",
    "top_families = family_counts[:20].index.tolist()\n",
    "\n",
    "# Create a list of sample sets and label sets for the top 20 families\n",
    "top_sample_sets = []\n",
    "\n",
    "# Create a list of label sets for the top 20 families\n",
    "top_label_sets = []\n",
    "\n",
    "# Append the sample and label sets for each family to the corresponding list\n",
    "for family in top_families:\n",
    "    top_sample_sets.append(training_sets_samples[family])\n",
    "    top_label_sets.append(training_sets_labels[family])\n",
    "\n",
    "# Train the ensemble SVM model\n",
    "ensemble_svm = Ensemble_SVM(epochs=2000, learning_rate=0.005)\n",
    "ensemble_svm.fit(top_families, top_sample_sets, top_label_sets, regularization_term=0)\n",
    "\n",
    "# Print the top 20 families used in the ensemble model\n",
    "print(\"Top 20 families used in the ensemble model:\")\n",
    "print(top_families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the accuracy of each model in the ensemble and the average prediction score\n",
    "model_accuracies = []\n",
    "average_prediction_score = []\n",
    "for index in range(len(top_families)):\n",
    "    # Get the predictions for each model\n",
    "    predictions = ensemble_svm.models[index].predict(top_sample_sets[index])\n",
    "    predictions = torch.where(predictions > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
    "    # Calculate the accuracy of the model\n",
    "    num_correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == top_label_sets[index][i]:\n",
    "            num_correct += 1\n",
    "    model_accuracies.append(num_correct / len(predictions))\n",
    "    # Calculate the average prediction score\n",
    "    average_prediction_score.append(torch.mean(predictions).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4904871144990607, 0.3083884137780568, 0.2637608957191304, 0.2532776624522216, 0.3019492267668099, 0.1659027385430322, 0.299253177828677, 0.26312019465538333, 0.2996086366725348, 0.28318515302000513, 0.39707613369873834, 0.20984725619907257, 0.18706181096821728, 0.3298559197979777, 0.25370344003898826, 0.3089317371179168, 0.28881429315562945, 0.19283983231494656, 0.15734280060569503, 0.2253569391353849]\n"
     ]
    }
   ],
   "source": [
    "# Adjust the bias using the average prediction score\n",
    "# Bias is multiplied to the model prediction\n",
    "# If an individual model has high accuracy but low average prediction score, increase the prediction score\n",
    "# If an individual model has low accuracy but high average prediction score, decrease the prediction score\n",
    "# The closer the ratio is to 1:1, the less bias is applied to the prediction\n",
    "bias = [model_accuracies[i] / average_prediction_score[i] / 5 for i in range(len(top_families))]\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Local\\Temp\\ipykernel_22304\\4094012881.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.sigmoid(torch.matmul(torch.tensor(samples, dtype=torch.float32), self.weights) + self.bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the ensemble model on the training set: 0.5328765318223745\n"
     ]
    }
   ],
   "source": [
    "# Get the accuracy of the ensemble model on the training set\n",
    "correct_predictions = 0\n",
    "for i in range(len(dataset_vectors)):\n",
    "    family = ensemble_svm.weighted_predict(torch.tensor(dataset_vectors[i], dtype=torch.float32), bias)\n",
    "    if family == dataset_families[i]:\n",
    "        correct_predictions += 1\n",
    "accuracy = correct_predictions / len(dataset_vectors)\n",
    "print(f\"\\nAccuracy of the ensemble model on the training set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Local\\Temp\\ipykernel_22304\\4094012881.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.sigmoid(torch.matmul(torch.tensor(samples, dtype=torch.float32), self.weights) + self.bias)\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix for the ensemble model\n",
    "confusion_matrix = np.zeros((len(top_families), len(top_families)), dtype=int)\n",
    "for i in range(len(dataset_vectors)):\n",
    "    if dataset_families[i] not in top_families:\n",
    "        continue\n",
    "    predicted_family = ensemble_svm.weighted_predict(torch.tensor(dataset_vectors[i], dtype=torch.float32), bias)\n",
    "    actual_family = dataset_families[i]\n",
    "    predicted_index = top_families.index(predicted_family)\n",
    "    actual_index = top_families.index(actual_family)\n",
    "    confusion_matrix[actual_index][predicted_index] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the confusion matrix to a csv file\n",
    "def save_confusion_matrix_to_csv(confusion_matrix, families, output_file_path):\n",
    "    df = pd.DataFrame(confusion_matrix, columns=families, index=families)\n",
    "    # Save the DataFrame to a csv file\n",
    "    df.to_csv(output_file_path)\n",
    "\n",
    "output_file_path = 'confusion_matrix.csv'\n",
    "save_confusion_matrix_to_csv(confusion_matrix, top_families, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    'solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate final model\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
