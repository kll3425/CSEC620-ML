{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded dataset from csv file:\n",
      "                                                hash  label     family  \\\n",
      "0  00002d74a9faa53f5199c910b652ef09d3a7f6bd42b693...      1  GinMaster   \n",
      "1  000068216bdb459df847bfdd67dd11069c3c50166db1ea...      0     benign   \n",
      "2  0000764713b286cfe7e8e76c7038c92312977712d9c5a8...      1     Opfake   \n",
      "3  0000962c2c34de1ca0c329b18be7847459da2d9d14b6b2...      0     benign   \n",
      "4  000167f1ff061ea91440c40659c11c2af160342fd2e493...      0     benign   \n",
      "\n",
      "                                              vector  \n",
      "0  [1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0...  \n",
      "1  [1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...  \n",
      "2  [1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0...  \n",
      "3  [1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0...  \n",
      "4  [1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0...  \n",
      "\n",
      "Keys of the loaded dataset:\n",
      "Index(['hash', 'label', 'family', 'vector'], dtype='object')\n",
      "\n",
      "First 5 samples in the dataset:\n",
      "Sample 0: 00002d74a9faa53f5199c910b652ef09d3a7f6bd42b693755a233635c3ffb0f4\n",
      "\t[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], 1, GinMaster\n",
      "Sample 1: 000068216bdb459df847bfdd67dd11069c3c50166db1ea8772cdc9250d948bcf\n",
      "\t[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0, benign\n",
      "Sample 2: 0000764713b286cfe7e8e76c7038c92312977712d9c5a86d504be54f3c1d025a\n",
      "\t[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 1, Opfake\n",
      "Sample 3: 0000962c2c34de1ca0c329b18be7847459da2d9d14b6b23a21cbc6427522403c\n",
      "\t[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0, benign\n",
      "Sample 4: 000167f1ff061ea91440c40659c11c2af160342fd2e493d609e4996b8820e78f\n",
      "\t[1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0, benign\n"
     ]
    }
   ],
   "source": [
    "# Import pandas libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the csv file\n",
    "def load_dataset_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Load the dataset from the csv file\n",
    "loaded_dataset = load_dataset_from_csv('vectorized_dataset.csv')\n",
    "print(\"\\nLoaded dataset from csv file:\")\n",
    "print(loaded_dataset.head())\n",
    "\n",
    "# Print keys of the loaded dataset\n",
    "print(\"\\nKeys of the loaded dataset:\")\n",
    "print(loaded_dataset.keys())\n",
    "\n",
    "# Convert all columns to numpy arrays\n",
    "dataset_hashes = loaded_dataset['hash'].tolist()\n",
    "dataset_labels = loaded_dataset['label'].tolist()\n",
    "dataset_families = loaded_dataset['family'].tolist()\n",
    "\n",
    "# Process each vector in the dataset by removing the brackets\n",
    "dataset_vectors = []\n",
    "for vector in loaded_dataset['vector']:\n",
    "    vector = vector.strip('[]').split(' ')\n",
    "    # Typecast the vector of strings to int\n",
    "    vector = [int(i) for i in vector]\n",
    "    dataset_vectors.append(vector)\n",
    "dataset_vectors = dataset_vectors\n",
    "\n",
    "# Print the first 5 samples in the dataset along with their hashes and labels\n",
    "print(\"\\nFirst 5 samples in the dataset:\")\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i}: {dataset_hashes[i]}\")\n",
    "    print(f\"\\t{dataset_vectors[i]}, {dataset_labels[i]}, {dataset_families[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of families in the dataset: 180\n",
      "\n",
      "Family counts in the dataset:\n",
      "benign: 123453\n",
      "FakeInstaller: 925\n",
      "DroidKungFu: 667\n",
      "Plankton: 625\n",
      "Opfake: 613\n",
      "GinMaster: 339\n",
      "BaseBridge: 330\n",
      "Iconosys: 152\n",
      "Kmin: 147\n",
      "FakeDoc: 132\n",
      "Geinimi: 92\n",
      "Adrd: 91\n",
      "DroidDream: 81\n",
      "ExploitLinuxLotoor: 70\n",
      "Glodream: 69\n",
      "MobileTx: 69\n",
      "FakeRun: 61\n",
      "SendPay: 59\n",
      "Gappusin: 58\n",
      "Imlog: 43\n",
      "SMSreg: 41\n",
      "Yzhc: 37\n",
      "Jifake: 29\n",
      "Hamob: 28\n",
      "Boxer: 27\n",
      "Fakelogo: 19\n",
      "Penetho: 19\n",
      "Nyleaker: 18\n",
      "Xsider: 18\n",
      "FakePlayer: 17\n",
      "Dougalek: 17\n",
      "Fatakr: 17\n",
      "Vdloader: 16\n",
      "FoCobers: 15\n",
      "Stealer: 14\n",
      "SerBG: 14\n",
      "Typstu: 14\n",
      "Mobilespy: 14\n",
      "Steek: 14\n",
      "Zitmo: 14\n",
      "Nandrobox: 13\n",
      "TrojanSMS.Hippo: 13\n",
      "Fakengry: 13\n",
      "SpyHasb: 13\n",
      "Copycat: 12\n",
      "FakeTimer: 12\n",
      "Nickspy: 12\n",
      "Placms: 12\n",
      "Cosha: 11\n",
      "DroidSheep: 11\n",
      "Spitmo: 11\n",
      "Biige: 10\n",
      "AccuTrack: 10\n",
      "SMSZombie: 10\n",
      "Raden: 10\n",
      "Kiser: 9\n",
      "Stiniter: 9\n",
      "Zsone: 8\n",
      "Mobinauten: 8\n",
      "Spyset: 8\n",
      "Coogos: 8\n",
      "BeanBot: 8\n",
      "Sakezon: 8\n",
      "RootSmart: 7\n",
      "Gapev: 7\n",
      "Ceshark: 7\n",
      "Gamex: 7\n",
      "Mania: 6\n",
      "Lemon: 6\n",
      "Ksapp: 6\n",
      "SeaWeth: 6\n",
      "Kidlogger: 6\n",
      "Fjcon: 6\n",
      "QPlus: 6\n",
      "Trackplus: 6\n",
      "Aks: 5\n",
      "FarMap: 5\n",
      "TrojanSMS.Denofow: 5\n",
      "Stealthcell: 5\n",
      "FaceNiff: 5\n",
      "SpyPhone: 5\n",
      "Luckycat: 5\n",
      "Vidro: 5\n",
      "Gonca: 5\n",
      "DroidRooter: 4\n",
      "PdaSpy: 4\n",
      "EICAR-Test-File: 4\n",
      "Nisev: 4\n",
      "Replicator: 4\n",
      "RediAssi: 3\n",
      "FakeFlash: 3\n",
      "GGtrack: 3\n",
      "Tapsnake: 3\n",
      "Generic: 3\n",
      "SmsWatcher: 3\n",
      "TigerBot: 3\n",
      "Hispo: 3\n",
      "SpyMob: 3\n",
      "LifeMon: 3\n",
      "Spyoo: 3\n",
      "Moghava: 3\n",
      "Fsm: 3\n",
      "Gmuse: 3\n",
      "FinSpy: 3\n",
      "Adsms: 3\n",
      "Fidall: 3\n",
      "Rooter: 3\n",
      "GPSpy: 3\n",
      "SpyBubble: 3\n",
      "Proreso: 2\n",
      "Foncy: 2\n",
      "SmForw: 2\n",
      "YcChar: 2\n",
      "Koomer: 2\n",
      "Fauxcopy: 2\n",
      "Dialer: 2\n",
      "Loozfon: 2\n",
      "Tesbo: 2\n",
      "SheriDroid: 2\n",
      "Ackposts: 2\n",
      "CgFinder: 2\n",
      "Anti: 2\n",
      "Dogowar: 2\n",
      "Dabom: 2\n",
      "CrWind: 2\n",
      "CellSpy: 2\n",
      "NickyRCP: 2\n",
      "Antares: 2\n",
      "TheftAware: 2\n",
      "JSmsHider: 2\n",
      "Pirates: 2\n",
      "Saiva: 2\n",
      "Flexispy: 2\n",
      "TrojanSMS.Boxer.AQ: 1\n",
      "SuBatt: 1\n",
      "Anudow: 1\n",
      "SMSSend: 1\n",
      "Fujacks: 1\n",
      "Maxit: 1\n",
      "MMarketPay: 1\n",
      "Gasms: 1\n",
      "Spy.ImLog: 1\n",
      "Arspam: 1\n",
      "Booster: 1\n",
      "SMSBomber: 1\n",
      "SmsSpy: 1\n",
      "Acnetdoor: 1\n",
      "TrojanSMS.Stealer: 1\n",
      "Lypro: 1\n",
      "Spy.GoneSixty: 1\n",
      "Fakeview: 1\n",
      "Mobsquz: 1\n",
      "Sdisp: 1\n",
      "GlodEagl: 1\n",
      "RuFraud: 1\n",
      "Sonus: 1\n",
      "Qicsom: 1\n",
      "FakeNefix: 1\n",
      "Ansca: 1\n",
      "Ssmsp: 1\n",
      "Exploit.RageCage: 1\n",
      "MTracker: 1\n",
      "JS/Exploit-DynSrc: 1\n",
      "Loicdos: 1\n",
      "RATC: 1\n",
      "UpdtKiller: 1\n",
      "SafeKidZone: 1\n",
      "Maistealer: 1\n",
      "Updtbot: 1\n",
      "CellShark: 1\n",
      "EWalls: 1\n",
      "Netisend: 1\n",
      "Cawitt: 1\n",
      "Whapsni: 1\n",
      "Faceniff: 1\n",
      "PJApps: 1\n",
      "Pirater: 1\n",
      "Bgserv: 1\n",
      "Smspacem: 1\n",
      "Bosm: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the number of families in the dataset\n",
    "print(\"\\nNumber of families in the dataset:\", len(set(dataset_families)))\n",
    "\n",
    "# Print each family in the dataset and its count, sorted by count\n",
    "family_counts = pd.Series(dataset_families).value_counts()\n",
    "print(\"\\nFamily counts in the dataset:\")\n",
    "for family, count in family_counts.items():\n",
    "    print(f\"{family}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training sets created: 55\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate a one-vs-all training set for each family in the dataset with more than n occurrences\n",
    "# Set to 0 to create a dataset for all families\n",
    "min_occurrences = 10\n",
    "\n",
    "# Create a dictionary to store the training sets for each family\n",
    "training_sets_samples = {}\n",
    "training_sets_labels = {}\n",
    "\n",
    "# Iterate through each family and create a one-vs-all training set\n",
    "for family in family_counts.index:\n",
    "    if family_counts[family] >= min_occurrences:\n",
    "        training_set_samples = []\n",
    "        training_set_labels = []\n",
    "        # Append malware samples for the current family\n",
    "        malware_indexes = [i for i in range(len(dataset_families)) if dataset_families[i] == family]\n",
    "        index = 0\n",
    "        while len(training_set_samples) < 50000:\n",
    "            training_set_samples.append(dataset_vectors[malware_indexes[index]])\n",
    "            training_set_labels.append(1)\n",
    "            index += 1\n",
    "            if index >= len(malware_indexes):\n",
    "                index = 0\n",
    "        # Append benign samples for the current family\n",
    "        benign_indexes = [i for i in range(len(dataset_families)) if dataset_families[i] != family]\n",
    "        index = 0\n",
    "        while len(training_set_samples) < 100000:\n",
    "            training_set_samples.append(dataset_vectors[benign_indexes[index]])\n",
    "            training_set_labels.append(0)\n",
    "            index += 1\n",
    "            if index >= len(benign_indexes):\n",
    "                index = 0\n",
    "        training_sets_samples[family] = training_set_samples\n",
    "        training_sets_labels[family] = training_set_labels\n",
    "\n",
    "# Print the number of training sets created\n",
    "print(\"\\nNumber of training sets created:\", len(training_sets_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of malware and benign samples in each training set:\n",
      "benign: 50000 malware samples, 50000 benign samples\n",
      "FakeInstaller: 50000 malware samples, 50000 benign samples\n",
      "DroidKungFu: 50000 malware samples, 50000 benign samples\n",
      "Plankton: 50000 malware samples, 50000 benign samples\n",
      "Opfake: 50000 malware samples, 50000 benign samples\n",
      "GinMaster: 50000 malware samples, 50000 benign samples\n",
      "BaseBridge: 50000 malware samples, 50000 benign samples\n",
      "Iconosys: 50000 malware samples, 50000 benign samples\n",
      "Kmin: 50000 malware samples, 50000 benign samples\n",
      "FakeDoc: 50000 malware samples, 50000 benign samples\n",
      "Geinimi: 50000 malware samples, 50000 benign samples\n",
      "Adrd: 50000 malware samples, 50000 benign samples\n",
      "DroidDream: 50000 malware samples, 50000 benign samples\n",
      "ExploitLinuxLotoor: 50000 malware samples, 50000 benign samples\n",
      "Glodream: 50000 malware samples, 50000 benign samples\n",
      "MobileTx: 50000 malware samples, 50000 benign samples\n",
      "FakeRun: 50000 malware samples, 50000 benign samples\n",
      "SendPay: 50000 malware samples, 50000 benign samples\n",
      "Gappusin: 50000 malware samples, 50000 benign samples\n",
      "Imlog: 50000 malware samples, 50000 benign samples\n",
      "SMSreg: 50000 malware samples, 50000 benign samples\n",
      "Yzhc: 50000 malware samples, 50000 benign samples\n",
      "Jifake: 50000 malware samples, 50000 benign samples\n",
      "Hamob: 50000 malware samples, 50000 benign samples\n",
      "Boxer: 50000 malware samples, 50000 benign samples\n",
      "Fakelogo: 50000 malware samples, 50000 benign samples\n",
      "Penetho: 50000 malware samples, 50000 benign samples\n",
      "Nyleaker: 50000 malware samples, 50000 benign samples\n",
      "Xsider: 50000 malware samples, 50000 benign samples\n",
      "FakePlayer: 50000 malware samples, 50000 benign samples\n",
      "Dougalek: 50000 malware samples, 50000 benign samples\n",
      "Fatakr: 50000 malware samples, 50000 benign samples\n",
      "Vdloader: 50000 malware samples, 50000 benign samples\n",
      "FoCobers: 50000 malware samples, 50000 benign samples\n",
      "Stealer: 50000 malware samples, 50000 benign samples\n",
      "SerBG: 50000 malware samples, 50000 benign samples\n",
      "Typstu: 50000 malware samples, 50000 benign samples\n",
      "Mobilespy: 50000 malware samples, 50000 benign samples\n",
      "Steek: 50000 malware samples, 50000 benign samples\n",
      "Zitmo: 50000 malware samples, 50000 benign samples\n",
      "Nandrobox: 50000 malware samples, 50000 benign samples\n",
      "TrojanSMS.Hippo: 50000 malware samples, 50000 benign samples\n",
      "Fakengry: 50000 malware samples, 50000 benign samples\n",
      "SpyHasb: 50000 malware samples, 50000 benign samples\n",
      "Copycat: 50000 malware samples, 50000 benign samples\n",
      "FakeTimer: 50000 malware samples, 50000 benign samples\n",
      "Nickspy: 50000 malware samples, 50000 benign samples\n",
      "Placms: 50000 malware samples, 50000 benign samples\n",
      "Cosha: 50000 malware samples, 50000 benign samples\n",
      "DroidSheep: 50000 malware samples, 50000 benign samples\n",
      "Spitmo: 50000 malware samples, 50000 benign samples\n",
      "Biige: 50000 malware samples, 50000 benign samples\n",
      "AccuTrack: 50000 malware samples, 50000 benign samples\n",
      "SMSZombie: 50000 malware samples, 50000 benign samples\n",
      "Raden: 50000 malware samples, 50000 benign samples\n"
     ]
    }
   ],
   "source": [
    "# Print the number of malware and benign samples in each training set\n",
    "print(\"\\nNumber of malware and benign samples in each training set:\")\n",
    "for family, labels in training_sets_labels.items():\n",
    "    print(f\"{family}: {labels.count(1)} malware samples, {labels.count(0)} benign samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 samples of the training set for family 'FakeInstaller':\n",
      "Sample 0: 00002d74a9faa53f5199c910b652ef09d3a7f6bd42b693755a233635c3ffb0f4\n",
      "\t[1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "Sample 1: 000068216bdb459df847bfdd67dd11069c3c50166db1ea8772cdc9250d948bcf\n",
      "\t[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "Sample 2: 0000764713b286cfe7e8e76c7038c92312977712d9c5a86d504be54f3c1d025a\n",
      "\t[1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "Sample 3: 0000962c2c34de1ca0c329b18be7847459da2d9d14b6b23a21cbc6427522403c\n",
      "\t[1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "Sample 4: 000167f1ff061ea91440c40659c11c2af160342fd2e493d609e4996b8820e78f\n",
      "\t[1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 samples of the training set for the family at index idx\n",
    "idx = 1\n",
    "family = family_counts.index[idx]\n",
    "print(f\"\\nFirst 5 samples of the training set for family '{family}':\")\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i}: {dataset_hashes[i]}\")\n",
    "    print(f\"\\t{training_sets_samples[family][i]}, {training_sets_labels[family][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ChatGPT was used to help convert Numpy SVM to PyTorch SVM using automatic optimizer\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, epochs=1000, learning_rate=0.001):\n",
    "        # Set learning rate and number of epochs/iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Initiate weights and biases to None, assigned based on the size of the first training point\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, samples, raw_labels, regularization_term):\n",
    "        # Convert samples and labels to PyTorch tensors\n",
    "        samples = torch.tensor(samples, dtype=torch.float32)\n",
    "        raw_labels = torch.tensor(raw_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Update labels from (0 or 1) to (-1 or 1) for hinge loss calculation\n",
    "        labels = torch.where(raw_labels == 1, torch.tensor(1.0, dtype=torch.float32), torch.tensor(-1.0, dtype=torch.float32))\n",
    "\n",
    "        # Get the number of samples and number of features per sample\n",
    "        num_samples, num_features = samples.shape\n",
    "\n",
    "        # Initialize n weights to 0, where n is the number of features\n",
    "        # requires_grad set to True to allow automatic tuning by PyTorch optimizer\n",
    "        self.weights = torch.zeros((num_features, 1), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Set the initial bias to 0 (tensor of size 1)\n",
    "        # requires_grad set to True to allow automatic tuning by PyTorch optimizer\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Specify Stocastic Gradient Descent as the optimizer\n",
    "        optimizer = torch.optim.SGD([self.weights, self.bias], lr=self.learning_rate)\n",
    "\n",
    "        # Training function\n",
    "        for epoch_index in range(self.epochs):\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predict the score of the sample (same as predict function w/o function call overhead)\n",
    "            prediction = torch.matmul(samples, self.weights) - self.bias\n",
    "\n",
    "            # Compute the margin by applying the labels to the corresponding predictions\n",
    "            margin = prediction * labels\n",
    "\n",
    "            # Compute the average hinge loss\n",
    "            hinge_loss = torch.mean(torch.clamp(1 - margin, min=0))\n",
    "\n",
    "            # Add regularization to the cost function\n",
    "            reg_cost = regularization_term * torch.norm(self.weights, p=2) / 2\n",
    "\n",
    "            # Calculate total loss\n",
    "            loss = hinge_loss + reg_cost\n",
    "\n",
    "            # Use PyTorch to automatically compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "    def predict(self, samples):\n",
    "        # Compute the score/prediction by finding the dot product between the data points and the weights, including the bias\n",
    "        return torch.sigmoid(torch.matmul(samples, self.weights) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained SVM model:\n",
      "Bias: tensor([0.4494], requires_grad=True)\n",
      "\n",
      "First 5 samples of the training set for family 'FakeInstaller' where the label is 1:\n",
      "Sample 0: 00002d74a9faa53f5199c910b652ef09d3a7f6bd42b693755a233635c3ffb0f4\n",
      "\t[1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "\tPrediction: tensor([0.8347], grad_fn=<SigmoidBackward0>)\n",
      "Sample 1: 000068216bdb459df847bfdd67dd11069c3c50166db1ea8772cdc9250d948bcf\n",
      "\t[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "\tPrediction: tensor([0.7988], grad_fn=<SigmoidBackward0>)\n",
      "Sample 2: 0000764713b286cfe7e8e76c7038c92312977712d9c5a86d504be54f3c1d025a\n",
      "\t[1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "\tPrediction: tensor([0.8697], grad_fn=<SigmoidBackward0>)\n",
      "Sample 3: 0000962c2c34de1ca0c329b18be7847459da2d9d14b6b23a21cbc6427522403c\n",
      "\t[1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "\tPrediction: tensor([0.8697], grad_fn=<SigmoidBackward0>)\n",
      "Sample 4: 000167f1ff061ea91440c40659c11c2af160342fd2e493d609e4996b8820e78f\n",
      "\t[1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n",
      "\tPrediction: tensor([0.8729], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "First 5 samples of the training set for family 'FakeInstaller' where the label is 0:\n",
      "Sample 50000: 63120521eae8dc4a1cd0df68b1a8df1c0b01d58522e874dd10a0fec2ce884f39\n",
      "\t[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], 0\n",
      "\tPrediction: tensor([0.4638], grad_fn=<SigmoidBackward0>)\n",
      "Sample 50001: 6312646f4c90c4578397362c06667d26efaec38eb2a55e0378879d614779b197\n",
      "\t[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0\n",
      "\tPrediction: tensor([0.5688], grad_fn=<SigmoidBackward0>)\n",
      "Sample 50002: 631387bd6baa9d1932c52fb278e03690376ef03d1ef2a56dd40e946423ba4f07\n",
      "\t[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 0\n",
      "\tPrediction: tensor([0.6837], grad_fn=<SigmoidBackward0>)\n",
      "Sample 50003: 631476c68f3867bd771765d0868f38fbdab5bfda1d9c05ed83b23b8ee8e4abab\n",
      "\t[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0\n",
      "\tPrediction: tensor([0.4741], grad_fn=<SigmoidBackward0>)\n",
      "Sample 50004: 6314ae717606dd9a69a1363877472b6f0332b1a87656838becba87d95b0c4b6c\n",
      "\t[1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0\n",
      "\tPrediction: tensor([0.4054], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create an SVM object\n",
    "svm = SVM(epochs=5000, learning_rate=0.001)\n",
    "\n",
    "# Train the SVM model using the training set for the family at index idx\n",
    "# The training set at idx 0 is benign\n",
    "idx = 1\n",
    "family = family_counts.index[idx]\n",
    "\n",
    "# Train the single class SVM model on the training set\n",
    "svm.fit(training_sets_samples[family], training_sets_labels[family], regularization_term=0.5)\n",
    "\n",
    "# Print the weights and bias of the trained SVM model\n",
    "print(\"\\nTrained SVM model:\")\n",
    "# print(f\"Weights: {svm.weights}\")\n",
    "print(f\"Bias: {svm.bias}\")\n",
    "\n",
    "# Print the first 5 samples of the training set for the family where the label is 1\n",
    "print(f\"\\nFirst 5 samples of the training set for family '{family}' where the label is 1:\")\n",
    "count = 0\n",
    "for i in range(len(training_sets_labels[family])):\n",
    "    if training_sets_labels[family][i] == 1:\n",
    "        print(f\"Sample {i}: {dataset_hashes[i]}\")\n",
    "        print(f\"\\t{training_sets_samples[family][i]}, {training_sets_labels[family][i]}\")\n",
    "        print(f\"\\tPrediction: {svm.predict(torch.tensor(training_sets_samples[family][i], dtype=torch.float32))}\")\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            break\n",
    "\n",
    "# Print the first 5 samples of the training set for the family where the label is 0\n",
    "print(f\"\\nFirst 5 samples of the training set for family '{family}' where the label is 0:\")\n",
    "count = 0\n",
    "for i in range(len(training_sets_labels[family])):\n",
    "    if training_sets_labels[family][i] == 0:\n",
    "        print(f\"Sample {i}: {dataset_hashes[i]}\")\n",
    "        print(f\"\\t{training_sets_samples[family][i]}, {training_sets_labels[family][i]}\")\n",
    "        print(f\"\\tPrediction: {svm.predict(torch.tensor(training_sets_samples[family][i], dtype=torch.float32))}\")\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
