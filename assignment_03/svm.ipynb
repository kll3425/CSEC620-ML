{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded dataset from csv file:\n",
      "                                                hash  label     family  \\\n",
      "0  00002d74a9faa53f5199c910b652ef09d3a7f6bd42b693...      1  GinMaster   \n",
      "1  000068216bdb459df847bfdd67dd11069c3c50166db1ea...      0     benign   \n",
      "2  0000764713b286cfe7e8e76c7038c92312977712d9c5a8...      1     Opfake   \n",
      "3  0000962c2c34de1ca0c329b18be7847459da2d9d14b6b2...      0     benign   \n",
      "4  000167f1ff061ea91440c40659c11c2af160342fd2e493...      0     benign   \n",
      "\n",
      "                                      vector  \n",
      "0  [1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0]  \n",
      "1  [1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]  \n",
      "2  [1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0]  \n",
      "3  [1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]  \n",
      "4  [1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]  \n",
      "\n",
      "Keys of the loaded dataset:\n",
      "Index(['hash', 'label', 'family', 'vector'], dtype='object')\n",
      "\n",
      "First 5 samples in the dataset:\n",
      "Sample 0: 00002d74a9faa53f5199c910b652ef09d3a7f6bd42b693755a233635c3ffb0f4\n",
      "\t[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1, GinMaster\n",
      "Sample 1: 000068216bdb459df847bfdd67dd11069c3c50166db1ea8772cdc9250d948bcf\n",
      "\t[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0, benign\n",
      "Sample 2: 0000764713b286cfe7e8e76c7038c92312977712d9c5a86d504be54f3c1d025a\n",
      "\t[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0], 1, Opfake\n",
      "Sample 3: 0000962c2c34de1ca0c329b18be7847459da2d9d14b6b23a21cbc6427522403c\n",
      "\t[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0, benign\n",
      "Sample 4: 000167f1ff061ea91440c40659c11c2af160342fd2e493d609e4996b8820e78f\n",
      "\t[1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0, benign\n"
     ]
    }
   ],
   "source": [
    "# Import pandas libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the csv file\n",
    "def load_dataset_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Load the dataset from the csv file\n",
    "loaded_dataset = load_dataset_from_csv('vectorized_dataset.csv')\n",
    "print(\"\\nLoaded dataset from csv file:\")\n",
    "print(loaded_dataset.head())\n",
    "\n",
    "# Print keys of the loaded dataset\n",
    "print(\"\\nKeys of the loaded dataset:\")\n",
    "print(loaded_dataset.keys())\n",
    "\n",
    "# Convert all columns to numpy arrays\n",
    "dataset_hashes = loaded_dataset['hash'].tolist()\n",
    "dataset_labels = loaded_dataset['label'].tolist()\n",
    "dataset_families = loaded_dataset['family'].tolist()\n",
    "\n",
    "# Process each vector in the dataset by removing the brackets\n",
    "dataset_vectors = []\n",
    "for vector in loaded_dataset['vector']:\n",
    "    vector = vector.strip('[]').split(' ')\n",
    "    # Typecast the vector of strings to int\n",
    "    vector = [int(i) for i in vector]\n",
    "    dataset_vectors.append(vector)\n",
    "dataset_vectors = dataset_vectors\n",
    "\n",
    "# Print the first 5 samples in the dataset along with their hashes and labels\n",
    "print(\"\\nFirst 5 samples in the dataset:\")\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i}: {dataset_hashes[i]}\")\n",
    "    print(f\"\\t{dataset_vectors[i]}, {dataset_labels[i]}, {dataset_families[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of families in the dataset: 180\n",
      "\n",
      "Family counts in the dataset:\n",
      "benign: 123453\n",
      "FakeInstaller: 925\n",
      "DroidKungFu: 667\n",
      "Plankton: 625\n",
      "Opfake: 613\n",
      "GinMaster: 339\n",
      "BaseBridge: 330\n",
      "Iconosys: 152\n",
      "Kmin: 147\n",
      "FakeDoc: 132\n",
      "Geinimi: 92\n",
      "Adrd: 91\n",
      "DroidDream: 81\n",
      "ExploitLinuxLotoor: 70\n",
      "Glodream: 69\n",
      "MobileTx: 69\n",
      "FakeRun: 61\n",
      "SendPay: 59\n",
      "Gappusin: 58\n",
      "Imlog: 43\n",
      "SMSreg: 41\n",
      "Yzhc: 37\n",
      "Jifake: 29\n",
      "Hamob: 28\n",
      "Boxer: 27\n",
      "Fakelogo: 19\n",
      "Penetho: 19\n",
      "Nyleaker: 18\n",
      "Xsider: 18\n",
      "FakePlayer: 17\n",
      "Dougalek: 17\n",
      "Fatakr: 17\n",
      "Vdloader: 16\n",
      "FoCobers: 15\n",
      "Stealer: 14\n",
      "SerBG: 14\n",
      "Typstu: 14\n",
      "Mobilespy: 14\n",
      "Steek: 14\n",
      "Zitmo: 14\n",
      "Nandrobox: 13\n",
      "TrojanSMS.Hippo: 13\n",
      "Fakengry: 13\n",
      "SpyHasb: 13\n",
      "Copycat: 12\n",
      "FakeTimer: 12\n",
      "Nickspy: 12\n",
      "Placms: 12\n",
      "Cosha: 11\n",
      "DroidSheep: 11\n",
      "Spitmo: 11\n",
      "Biige: 10\n",
      "AccuTrack: 10\n",
      "SMSZombie: 10\n",
      "Raden: 10\n",
      "Kiser: 9\n",
      "Stiniter: 9\n",
      "Zsone: 8\n",
      "Mobinauten: 8\n",
      "Spyset: 8\n",
      "Coogos: 8\n",
      "BeanBot: 8\n",
      "Sakezon: 8\n",
      "RootSmart: 7\n",
      "Gapev: 7\n",
      "Ceshark: 7\n",
      "Gamex: 7\n",
      "Mania: 6\n",
      "Lemon: 6\n",
      "Ksapp: 6\n",
      "SeaWeth: 6\n",
      "Kidlogger: 6\n",
      "Fjcon: 6\n",
      "QPlus: 6\n",
      "Trackplus: 6\n",
      "Aks: 5\n",
      "FarMap: 5\n",
      "TrojanSMS.Denofow: 5\n",
      "Stealthcell: 5\n",
      "FaceNiff: 5\n",
      "SpyPhone: 5\n",
      "Luckycat: 5\n",
      "Vidro: 5\n",
      "Gonca: 5\n",
      "DroidRooter: 4\n",
      "PdaSpy: 4\n",
      "EICAR-Test-File: 4\n",
      "Nisev: 4\n",
      "Replicator: 4\n",
      "RediAssi: 3\n",
      "FakeFlash: 3\n",
      "GGtrack: 3\n",
      "Tapsnake: 3\n",
      "Generic: 3\n",
      "SmsWatcher: 3\n",
      "TigerBot: 3\n",
      "Hispo: 3\n",
      "SpyMob: 3\n",
      "LifeMon: 3\n",
      "Spyoo: 3\n",
      "Moghava: 3\n",
      "Fsm: 3\n",
      "Gmuse: 3\n",
      "FinSpy: 3\n",
      "Adsms: 3\n",
      "Fidall: 3\n",
      "Rooter: 3\n",
      "GPSpy: 3\n",
      "SpyBubble: 3\n",
      "Proreso: 2\n",
      "Foncy: 2\n",
      "SmForw: 2\n",
      "YcChar: 2\n",
      "Koomer: 2\n",
      "Fauxcopy: 2\n",
      "Dialer: 2\n",
      "Loozfon: 2\n",
      "Tesbo: 2\n",
      "SheriDroid: 2\n",
      "Ackposts: 2\n",
      "CgFinder: 2\n",
      "Anti: 2\n",
      "Dogowar: 2\n",
      "Dabom: 2\n",
      "CrWind: 2\n",
      "CellSpy: 2\n",
      "NickyRCP: 2\n",
      "Antares: 2\n",
      "TheftAware: 2\n",
      "JSmsHider: 2\n",
      "Pirates: 2\n",
      "Saiva: 2\n",
      "Flexispy: 2\n",
      "TrojanSMS.Boxer.AQ: 1\n",
      "SuBatt: 1\n",
      "Anudow: 1\n",
      "SMSSend: 1\n",
      "Fujacks: 1\n",
      "Maxit: 1\n",
      "MMarketPay: 1\n",
      "Gasms: 1\n",
      "Spy.ImLog: 1\n",
      "Arspam: 1\n",
      "Booster: 1\n",
      "SMSBomber: 1\n",
      "SmsSpy: 1\n",
      "Acnetdoor: 1\n",
      "TrojanSMS.Stealer: 1\n",
      "Lypro: 1\n",
      "Spy.GoneSixty: 1\n",
      "Fakeview: 1\n",
      "Mobsquz: 1\n",
      "Sdisp: 1\n",
      "GlodEagl: 1\n",
      "RuFraud: 1\n",
      "Sonus: 1\n",
      "Qicsom: 1\n",
      "FakeNefix: 1\n",
      "Ansca: 1\n",
      "Ssmsp: 1\n",
      "Exploit.RageCage: 1\n",
      "MTracker: 1\n",
      "JS/Exploit-DynSrc: 1\n",
      "Loicdos: 1\n",
      "RATC: 1\n",
      "UpdtKiller: 1\n",
      "SafeKidZone: 1\n",
      "Maistealer: 1\n",
      "Updtbot: 1\n",
      "CellShark: 1\n",
      "EWalls: 1\n",
      "Netisend: 1\n",
      "Cawitt: 1\n",
      "Whapsni: 1\n",
      "Faceniff: 1\n",
      "PJApps: 1\n",
      "Pirater: 1\n",
      "Bgserv: 1\n",
      "Smspacem: 1\n",
      "Bosm: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the number of families in the dataset\n",
    "print(\"\\nNumber of families in the dataset:\", len(set(dataset_families)))\n",
    "\n",
    "# Print each family in the dataset and its count, sorted by count\n",
    "family_counts = pd.Series(dataset_families).value_counts()\n",
    "print(\"\\nFamily counts in the dataset:\")\n",
    "for family, count in family_counts.items():\n",
    "    print(f\"{family}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training sets created: 55\n"
     ]
    }
   ],
   "source": [
    "# Generate a one-vs-all training set for each family in the dataset with more than n occurrences\n",
    "# Set to 0 to create a dataset for all families\n",
    "min_occurrences = 10\n",
    "\n",
    "# Create a dictionary to store the training sets for each family\n",
    "training_sets_samples = {}\n",
    "training_sets_labels = {}\n",
    "\n",
    "# Iterate through each family and create a one-vs-all training set\n",
    "for family in family_counts.index:\n",
    "    if family_counts[family] >= min_occurrences:\n",
    "        training_set_samples = []\n",
    "        training_set_labels = []\n",
    "        for i in range(len(dataset_families)):\n",
    "            if dataset_families[i] == family:\n",
    "                training_set_samples.append(dataset_vectors[i])\n",
    "                training_set_labels.append(1)\n",
    "            else:\n",
    "                training_set_samples.append(dataset_vectors[i])\n",
    "                training_set_labels.append(0)\n",
    "        training_sets_samples[family] = training_set_samples\n",
    "        training_sets_labels[family] = training_set_labels\n",
    "\n",
    "# Print the number of training sets created\n",
    "print(\"\\nNumber of training sets created:\", len(training_sets_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 samples of the training set for family 'FakeInstaller':\n",
      "Sample 0: 00002d74a9faa53f5199c910b652ef09d3a7f6bd42b693755a233635c3ffb0f4\n",
      "\t[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0\n",
      "Sample 1: 000068216bdb459df847bfdd67dd11069c3c50166db1ea8772cdc9250d948bcf\n",
      "\t[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0\n",
      "Sample 2: 0000764713b286cfe7e8e76c7038c92312977712d9c5a86d504be54f3c1d025a\n",
      "\t[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0], 0\n",
      "Sample 3: 0000962c2c34de1ca0c329b18be7847459da2d9d14b6b23a21cbc6427522403c\n",
      "\t[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0\n",
      "Sample 4: 000167f1ff061ea91440c40659c11c2af160342fd2e493d609e4996b8820e78f\n",
      "\t[1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 samples of the training set for the family at index idx\n",
    "idx = 1\n",
    "family = family_counts.index[idx]\n",
    "print(f\"\\nFirst 5 samples of the training set for family '{family}':\")\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i}: {dataset_hashes[i]}\")\n",
    "    print(f\"\\t{training_sets_samples[family][i]}, {training_sets_labels[family][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ChatGPT was used to help convert Numpy SVM to PyTorch SVM using automatic optimizer\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, epochs=100, learning_rate=0.001):\n",
    "        # Set learning rate and number of epochs/iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Initiate weights and biases to None, assigned based on the size of the first training point\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, samples, raw_labels, regularization_term):\n",
    "        # Convert samples and labels to PyTorch tensors\n",
    "        samples = torch.tensor(samples, dtype=torch.float32)\n",
    "        raw_labels = torch.tensor(raw_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Update labels from (0 or 1) to (-1 or 1)\n",
    "        labels = torch.where(raw_labels == 0, -1, 1)\n",
    "\n",
    "        # Get the number of samples and number of features per sample\n",
    "        num_samples, num_features = samples.shape\n",
    "\n",
    "        # Initialize n weights to 0, where n is the number of features\n",
    "        # requires_grad set to True to allow automatic tuning by PyTorch optimizer\n",
    "        self.weights = torch.zeros((num_features, 1), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Set the initial bias to 0\n",
    "        # requires_grad set to True to allow automatic tuning by PyTorch optimizer\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Specify Stocastic Gradient Descent as the optimizer\n",
    "        optimizer = torch.optim.SGD([self.weights, self.bias], lr=self.learning_rate)\n",
    "\n",
    "        # Training function\n",
    "        for epoch_index in range(self.epochs):\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predict the score of the sample (same as predict function w/o function call overhead)\n",
    "            prediction = torch.matmul(samples, self.weights) - self.bias\n",
    "\n",
    "            # Compute the margin by applying the labels to the corresponding predictions\n",
    "            margin = prediction * labels\n",
    "\n",
    "            # Compute the average hinge loss\n",
    "            hinge_loss = torch.mean(torch.clamp(1 - margin, min=0))\n",
    "\n",
    "            # Add regularization to the cost function\n",
    "            reg_cost = regularization_term * torch.norm(self.weights, p=2) / 2\n",
    "\n",
    "            # Calculate total loss\n",
    "            loss = hinge_loss + reg_cost\n",
    "\n",
    "            # Use PyTorch to automatically compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "    def predict(self, samples):\n",
    "        # Compute the score/prediction by finding the dot product between the data points and the weights, subtracting the bias\n",
    "        return torch.matmul(samples, self.weights) - self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM for family 'FakeInstaller' with 129013 samples\n",
      "Number of features in the training set: 20\n",
      "[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "\n",
      "Trained SVM model:\n",
      "Weights: tensor([[-0.0997],\n",
      "        [-0.0839],\n",
      "        [-0.0388],\n",
      "        [-0.0379],\n",
      "        [-0.0523],\n",
      "        [-0.0158],\n",
      "        [-0.0061],\n",
      "        [-0.0089],\n",
      "        [-0.0139],\n",
      "        [-0.0141],\n",
      "        [-0.0417],\n",
      "        [-0.0293],\n",
      "        [-0.0234],\n",
      "        [-0.0233],\n",
      "        [-0.0225],\n",
      "        [-0.0225],\n",
      "        [-0.0199],\n",
      "        [-0.0151],\n",
      "        [-0.0131],\n",
      "        [-0.0091]], requires_grad=True)\n",
      "Bias: tensor([0.1000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create an SVM object\n",
    "svm = SVM(epochs=100, learning_rate=0.001)\n",
    "\n",
    "# Train the SVM model using the training set for the family at index idx\n",
    "# The training set at idx 0 is benign\n",
    "idx = 1\n",
    "family = family_counts.index[idx]\n",
    "\n",
    "# Print the family and its count\n",
    "print(f\"\\nTraining SVM for family '{family}' with {len(training_sets_samples[family])} samples\")\n",
    "\n",
    "# Print the number of features in the training set\n",
    "num_features = len(training_sets_samples[family][0])\n",
    "print(f\"Number of features in the training set: {num_features}\")\n",
    "\n",
    "# Test prints\n",
    "print(training_sets_samples[family][0], training_sets_labels[family][0])\n",
    "\n",
    "# Train the single class SVM model on the training set\n",
    "svm.fit(training_sets_samples[family], training_sets_labels[family][0], regularization_term=0)\n",
    "\n",
    "# Print the weights and bias of the trained SVM model\n",
    "print(\"\\nTrained SVM model:\")\n",
    "print(f\"Weights: {svm.weights}\")\n",
    "print(f\"Bias: {svm.bias}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
